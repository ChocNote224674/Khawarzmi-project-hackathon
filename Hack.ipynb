{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (1.58.1)\n",
      "Requirement already satisfied: langchain in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (2.10.4)\n",
      "Requirement already satisfied: sniffio in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (2.2.0)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youssef\\AppData\\Local\\Temp\\ipykernel_18488\\1653654353.py:4: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(openai_api_key=\"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\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "La capitale de la France est Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Remplacez 'your-api-key' par votre clé API OpenAI\n",
    "llm = OpenAI(openai_api_key=)\n",
    "\n",
    "# Utilisation de .invoke() pour poser une question\n",
    "response = llm.invoke(\"Quelle est la capitale de la France ?\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': {'language': 'fr', 'question': 'What is the capital of France?', 'text': ''}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Remplacez par votre clé API OpenAI\n",
    "llm = OpenAI(openai_api_key=)\n",
    "\n",
    "# Exemple de données d'entrée (à adapter selon vos besoins)\n",
    "user_input = {\n",
    "    \"language\": \"fr\",  # Langue dans laquelle la question doit être traduite\n",
    "    \"question\": \"What is the capital of France?\",  # La question à traduire\n",
    "}\n",
    "\n",
    "# Création du prompt avec l'objectif de traduction uniquement\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"INSTRUCTIONS:  \n",
    "                1. Translate the following question into {language} without providing an answer.\n",
    "                2. Do not respond to the question, just translate it into the specified language.\n",
    "            QUESTION:\n",
    "            {question}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"The query is: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Création de la chaîne (chain) avec le modèle LLM\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Exécution de la chaîne avec les données d'entrée\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"language\": user_input[\"language\"],\n",
    "        \"question\": user_input[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Affichage de la réponse (traduction de la question)\n",
    "print({\"message\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': '\\n\\nFrench: Quelle est la capitale de la France?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Remplacez par votre clé API OpenAI\n",
    "llm = OpenAI(openai_api_key=)\n",
    "\n",
    "# Exemple de données d'entrée (à adapter selon vos besoins)\n",
    "user_input = {\n",
    "    \"language\": \"fr\",  # Langue dans laquelle la question doit être traduite\n",
    "    \"question\": \"What is the capital of France?\",  # La question à traduire\n",
    "}\n",
    "\n",
    "# Création du prompt avec l'objectif de traduction uniquement\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"INSTRUCTIONS:  \n",
    "                1. Translate the following question into {language} without providing an answer.\n",
    "                2. Do not respond to the question, just translate it into the specified language.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"QUESTION: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Création de la chaîne (chain) avec le modèle LLM\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Exécution de la chaîne avec les données d'entrée\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"language\": user_input[\"language\"],\n",
    "        \"question\": user_input[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Affichage de la réponse (traduction de la question)\n",
    "print({\"message\": answer['text']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Avez-vous déjà entendu parler de l'auto-examen des seins pour détecter le cancer du sein ?\n",
      "2. Avez-vous remarqué des changements dans la taille ou la forme de vos seins récemment ?\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Récupérer la clé API depuis le fichier .env\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Vérifier si la clé API est bien chargée\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'est pas configurée. Assurez-vous que 'OPENAI_API_KEY' est définie dans le fichier .env.\")\n",
    "\n",
    "def generate_questions_from_pdf(filepath):\n",
    "    # Extraire le texte du fichier PDF\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        content = \"\"\n",
    "        for page in pdf.pages:\n",
    "            content += page.extract_text()\n",
    "\n",
    "    if not content.strip():\n",
    "        return \"Aucun texte extrait du PDF.\"\n",
    "\n",
    "    # Utiliser l'API OpenAI pour générer des questions\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # Utilisation du bon modèle\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu es un assistant médical. Analyse les pdfs et génère un questionnaire pour une patiente réalisant une self-examination de la poitrine pour pouvoir voir si elle a quelques symptomes elle est au courant du cancer de sein tu dois la guider pour voir si il ya des symptomes .\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Voici le contenu scientifique :\\n\\n{content} inspirez vous pour trouvez des questions qui va l'aider a faire l'auto palpation\"}\n",
    "        ],\n",
    "        temperature=0.7, # Vous pouvez ajuster ce paramètre si nécessaire\n",
    "    )\n",
    "\n",
    "    # Extraire les questions de la réponse\n",
    "    questions = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return questions\n",
    "\n",
    "# Appeler la fonction avec votre fichier PDF\n",
    "try:\n",
    "    question = generate_questions_from_pdf(r\"D:\\GenAI Hackathon\\DATA\\Awareness about Breast Cancer and Breast Self-Examination.pdf\")\n",
    "    print(question)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la génération des questions : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lors de votre auto-examen, avez-vous senti une présence inhabituelle comme une masse ou un nodule dans vos seins ?\n",
      "2. Si oui, pourriez-vous me donner une description de ces masses ? Se déplacent-elles lorsque vous les touchez ? Sont-elles plutôt dures ou souples ?\n",
      "3. Lors de votre auto-examen, avez-vous eu une sensation d'inconfort tel que de la douleur ou une sensibilité accrue dans vos seins ?\n",
      "4. Au cours des dernières semaines, avez-vous remarqué un changement dans la taille ou la forme de vos seins ?\n",
      "5. Durant votre auto-examen, avez-vous observé des modifications au niveau de la peau de vos seins ? Peut-être une rougeur, une texture différente ou une éruption ?\n",
      "6. Avez-vous noté un changement dans l'apparence de vos mamelons, par exemple, sont-ils rentrés ou de forme différente ?\n",
      "7. Avez-vous constaté un écoulement inhabituel au niveau de vos mamelons ?\n",
      "8. Avez-vous ressenti une douleur qui persiste même après votre auto-examen ?\n",
      "9. Par rapport à vos auto-examens précédents, avez-vous remarqué l'apparition de nouveaux symptômes ou un changement dans ceux que vous aviez déjà ?\n",
      "10. Avez-vous l'impression que vos symptômes évoluent ou se modifient au fur et à mesure du temps ?\n",
      "11. Avez-vous récemment modifié votre contraception, suivi un traitement hormonal ou apporté des modifications à vos traitements médicaux habituels ? \n",
      "\n",
      "Rappelez-vous, il est très important de partager toute inquiétude avec votre médecin. Ces questions sont destinées à aider dans l'auto-examen, mais ne remplacent en aucun cas une consultation médicale.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Récupérer la clé API depuis le fichier .env\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Vérifier si la clé API est bien chargée\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'est pas configurée ou est incorrecte. Assurez-vous que 'OPENAI_API_KEY' est définie dans le fichier .env.\")\n",
    "\n",
    "# Prompt correct\n",
    "# Nouveau prompt sans apostrophes, plus humain et empathique\n",
    "prompt = \"\"\"\n",
    "Tu es un assistant médical bienveillant. Après qu'une patiente ait effectué une auto-palpation de ses seins, tu vas lui poser une série de questions pour savoir si elle a remarqué des anomalies ou des symptômes qui pourraient nécessiter une consultation médicale. Les questions doivent être formulées de manière simple, compréhensive et rassurante, en tenant compte de la sensibilité du sujet eviter les mots genre aggraver ect. Voici les aspects sur lesquels tu vas t'intéresser :\n",
    "\n",
    "1. Avez-vous remarqué des masses ou nodules dans vos seins pendant l'examen ?\n",
    "2. Si oui, comment décririez-vous ces masses ? Sont-elles mobiles, fixes, dures ou molles ?\n",
    "3. Avez-vous ressenti de la douleur ou de la sensibilité dans vos seins pendant l'examen ?\n",
    "4. Avez-vous remarqué des changements dans la taille ou la forme de vos seins récemment ?\n",
    "5. Avez-vous observé des changements dans l'apparence de la peau, comme des rougeurs, des ondulations ou des éruptions ?\n",
    "6. Avez-vous remarqué un changement dans l'aspect de vos mamelons, comme une inversion ou une déformation ?\n",
    "7. Avez-vous eu un écoulement anormal des mamelons ?\n",
    "8. Avez-vous ressenti de la douleur persistante après la palpation ?\n",
    "9. Par rapport aux auto-palpations précédentes, avez-vous observé des symptômes nouveaux ou différents ?\n",
    "10. Avez-vous remarqué que vos symptômes évoluent, par exemple, s'aggravent-ils ?\n",
    "12. Avez-vous récemment changé de méthode contraceptive, suivi un traitement hormonal ou modifié vos traitements médicaux ?\n",
    "\n",
    "Les questions doivent être posées de manière rassurante, sans jugement, afin de mettre la patiente à l'aise et de l'encourager à donner des réponses complètes. vous devait prendre en consideration les symptomes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_questions_from_pdf(promt):\n",
    "    # Utiliser l'API OpenAI pour générer des questions\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Voici le contenu scientifique : inspirez-vous pour trouver des questions qui vont l'aider à faire l'auto-palpation\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extraire les questions de la réponse\n",
    "    questions = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return questions\n",
    "\n",
    "print(generate_questions_from_pdf(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles disponibles :\n",
      "- gpt-4o-audio-preview-2024-10-01\n",
      "- gpt-4o-realtime-preview\n",
      "- gpt-4o-realtime-preview-2024-10-01\n",
      "- o1-mini-2024-09-12\n",
      "- dall-e-2\n",
      "- gpt-4-turbo\n",
      "- gpt-4-1106-preview\n",
      "- gpt-4o-2024-08-06\n",
      "- gpt-4o\n",
      "- gpt-3.5-turbo\n",
      "- gpt-3.5-turbo-0125\n",
      "- gpt-3.5-turbo-instruct\n",
      "- babbage-002\n",
      "- davinci-002\n",
      "- whisper-1\n",
      "- dall-e-3\n",
      "- text-embedding-3-small\n",
      "- gpt-3.5-turbo-16k\n",
      "- gpt-4-0125-preview\n",
      "- gpt-4-turbo-preview\n",
      "- omni-moderation-latest\n",
      "- gpt-4o-2024-05-13\n",
      "- omni-moderation-2024-09-26\n",
      "- tts-1-hd-1106\n",
      "- chatgpt-4o-latest\n",
      "- gpt-4\n",
      "- gpt-4-0613\n",
      "- o1-mini\n",
      "- o1-preview\n",
      "- o1-preview-2024-09-12\n",
      "- tts-1-hd\n",
      "- text-embedding-ada-002\n",
      "- gpt-3.5-turbo-1106\n",
      "- gpt-4o-audio-preview\n",
      "- tts-1\n",
      "- gpt-4o-mini\n",
      "- tts-1-1106\n",
      "- gpt-3.5-turbo-instruct-0914\n",
      "- gpt-4o-mini-2024-07-18\n",
      "- text-embedding-3-large\n",
      "- gpt-4o-realtime-preview-2024-12-17\n",
      "- gpt-4o-mini-realtime-preview\n",
      "- gpt-4o-mini-realtime-preview-2024-12-17\n",
      "- gpt-4o-2024-11-20\n",
      "- gpt-4o-audio-preview-2024-12-17\n",
      "- gpt-4o-mini-audio-preview\n",
      "- gpt-4o-mini-audio-preview-2024-12-17\n",
      "- gpt-4-turbo-2024-04-09\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configurer la clé API OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"\n",
    "    Liste tous les modèles disponibles pour l'utilisateur avec la clé API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = openai.Model.list()\n",
    "        print(\"Modèles disponibles :\")\n",
    "        for model in models[\"data\"]:\n",
    "            print(f\"- {model['id']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des modèles : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_fichier(fichier_entree):\n",
    "    \"\"\"\n",
    "    Lit un fichier texte et retourne son contenu.\n",
    "    \"\"\"\n",
    "    with open(fichier_entree, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def generate_medical_report(data):\n",
    "    \"\"\"\n",
    "    Génère un rapport médical structuré à partir des données fournies.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Voici les réponses d'un questionnaire d'auto-palpation mammaire :\\n\\n{data}\\n\\n la  patiente n'est pas forcement atteintes du cancer c'est juste un self check\n",
    "    Génère un rapport médical clair et structuré destiné à être envoyé au médecin, en incluant des recommandations ou observations nécessaires.\n",
    "    \"\"\"\n",
    "   \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Vous êtes un assistant médical aidant à générer des rapports pour prevenir le cancer du sein.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "   \n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Rapport de Questionnaire d'Auto-Palpation Mammaire\\n\\n#### Informations Générales\\n\\n- **Âge du patient :** 35 ans\\n- **Antécédents familiaux de cancer du sein :** Présents\\n- **Mode de vie :** Pas de consommation de tabac ou d'alcool.\\n\\n#### Résultats de l'Auto-Palpation\\n\\n- **Présence d'une masse ou d'un épaississement :** Masse détectée dans un sein.\\n- **Changements de taille ou de forme :** Aucun changement constaté.\\n- **Signes d’irritation cutanée :** Aucune rougeur, éruption cutanée ou irritation.\\n- **Douleur ou sensibilité :** Sensibilité ressentie dans un sein.\\n- **Changement de l’aspect du mamelon :** Aucun changement observé.\\n- **Changements dans les ganglions lymphatiques :** Aucun changement remarqué.\\n- **Présence de zones dures ou gonflées :** Zones dures détectées au toucher.\\n\\n#### Fréquence d'Auto-Palpation\\n\\n- **Fréquence :** Mensuelle\\n- **Antécédents médicaux ou diagnostics récents liés aux seins :** Aucun\\n\\n#### Commentaires Supplémentaires\\n\\nUne masse a été détectée dans le sein gauche sans douleur intense. La patiente a prévu une consultation médicale dans les prochains jours.\\n\\n#### Recommandations\\n\\nÉtant donné les antécédents familiaux de cancer du sein et la détection d'une masse lors de l'auto-palpation, il est fortement recommandé d'effectuer un dépistage diagnostique dès que possible.\\n\\nIl est également recommandé de maintenir la pratique régulière de l'auto-palpation et de signaler immédiatement tout changement observé.\\n\\n**Note :** Ce rapport est basé sur les informations fournies par la patiente et ne doit pas remplacer un examen clinique ou un diagnostic médical complet.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_medical_report(lire_fichier(r\"D:\\GenAI Hackathon\\reponse.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Récupérer la clé API depuis le fichier .env\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'a pas été trouvée dans le fichier .env.\")\n",
    "\n",
    "# Étape 1 : Chargement des fichiers PDF et extraction du texte\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    docs = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "            docs.extend(loader.load_and_split())\n",
    "    return docs\n",
    "\n",
    "# Étape 2 : Création des embeddings\n",
    "def create_embeddings(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    vector_store = Chroma.from_documents(texts, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Étape 3 : Génération de questions\n",
    "def generate_questions(vector_store, query, num_questions=10):\n",
    "    # Recherche dans le vecteur pour trouver le contexte pertinent\n",
    "    docs = vector_store.similarity_search(query, k=4)\n",
    "    relevant_text = \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Créer le prompt avec PromptTemplate\n",
    "    prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"num_questions\"],\n",
    "    template=\"\"\"You are a medical assistant AI tasked with generating questions for a breast cancer monitoring application. \n",
    "    These questions will be asked monthly to a patient for regular follow-up and to monitor potential changes in breast health. \n",
    "    Focus on symptoms, physical changes, or observations that could indicate a risk of breast cancer.\n",
    "\n",
    "    Use the context provided below to create {num_questions} clear, specific, and medically relevant questions suitable for monthly follow-ups.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTIONS:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "    # Configurer l'instance de LLM\n",
    "    llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "    # Créer une chaîne avec le prompt et le modèle\n",
    "    chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "\n",
    "    # Générer les questions\n",
    "    questions = chain.run({\"context\": relevant_text, \"num_questions\": num_questions})\n",
    "    return questions\n",
    "\n",
    "# Pipeline complet\n",
    "def main():\n",
    "    folder_path = \"DATA\"\n",
    "    query = \"Generate questions for breast cancer risk self-assessment.\"\n",
    "    \n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    docs = extract_text_from_pdfs(folder_path)\n",
    "    \n",
    "    print(\"Creating embeddings...\")\n",
    "    vector_store = create_embeddings(docs)\n",
    "    \n",
    "    print(\"Generating questions...\")\n",
    "    questions = generate_questions(vector_store, query)\n",
    "    print(\"Generated Questions:\")\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (2.10.4)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: fastapi>=0.95.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (2.2.0)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.20.1-cp312-cp312-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.68.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.0.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
      "Requirement already satisfied: anyio in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: certifi in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.2)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.3-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-14.1-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\genai hackathon\\mon_environnement\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "   ---------------------------------------- 0.0/628.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/628.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 628.3/628.3 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached grpcio-1.68.1-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.0.1-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading onnxruntime-1.20.1-cp312-cp312-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.3 MB 3.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 3.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/11.3 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.4/11.3 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/11.3 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.3 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.3 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.3 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.3 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading watchfiles-1.0.3-cp312-cp312-win_amd64.whl (285 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-14.1-cp312-cp312-win_amd64.whl (163 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.2 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.6-cp312-cp312-win_amd64.whl size=155082 sha256=817d4152db9176d040fd78c52c9085030a463e68329489ad6c89e6189c378b93\n",
      "  Stored in directory: c:\\users\\youssef\\appdata\\local\\pip\\cache\\wheels\\28\\29\\0e\\934c768c2e673547ec6e947e821346f4ed691a089fe046743f\n",
      "Successfully built chroma-hnswlib\n",
      "Installing collected packages: pypika, mpmath, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, sympy, shellingham, pyreadline3, pyproject_hooks, pyasn1, overrides, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, importlib-resources, httptools, grpcio, googleapis-common-protos, fsspec, filelock, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, importlib-metadata, humanfriendly, huggingface-hub, deprecated, build, typer, tokenizers, opentelemetry-api, google-auth, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 deprecated-1.2.15 durationpy-0.9 filelock-3.16.1 flatbuffers-24.3.25 fsspec-2024.12.0 google-auth-2.37.0 googleapis-common-protos-1.66.0 grpcio-1.68.1 httptools-0.6.4 huggingface-hub-0.27.0 humanfriendly-10.0 importlib-metadata-8.5.0 importlib-resources-6.4.5 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rsa-4.9 shellingham-1.5.4 sympy-1.13.3 tokenizers-0.20.3 typer-0.15.1 watchfiles-1.0.3 websocket-client-1.8.0 websockets-14.1 wrapt-1.17.0 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDFs...\n",
      "Creating embeddings...\n",
      "Generating questions...\n",
      "Generated Questions:\n"
     ]
    }
   ],
   "source": [
    "reponses = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Have you noticed any changes in the size or shape of your breasts since your last check-up?\n",
      "2. Do you experience any pain or discomfort in your breasts?\n",
      "3. Have you noticed any lumps or bumps in your breasts that were not there before?\n",
      "4. Are you aware of any family history of breast cancer?\n",
      "5. Do you perform regular breast self-examinations? \n",
      "6. Have you noticed any changes in the appearance of your nipples, such as inversion or discharge?\n",
      "7. Have you noticed any changes in skin texture or color on your breasts?\n",
      "8. Do you experience any persistent itching or rash on your breasts?\n",
      "9. Have you experienced any changes in your menstrual cycle or menopausal symptoms?\n",
      "10. Are you currently taking any hormone replacement therapy or birth control pills?\n"
     ]
    }
   ],
   "source": [
    "print(reponses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [q.strip() for q in reponses.strip().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Have you noticed any changes in the size or shape of your breasts since your last check-up?', '2. Do you experience any pain or discomfort in your breasts?', '3. Have you noticed any lumps or bumps in your breasts that were not there before?', '4. Are you aware of any family history of breast cancer?', '5. Do you perform regular breast self-examinations?', '6. Have you noticed any changes in the appearance of your nipples, such as inversion or discharge?', '7. Have you noticed any changes in skin texture or color on your breasts?', '8. Do you experience any persistent itching or rash on your breasts?', '9. Have you experienced any changes in your menstrual cycle or menopausal symptoms?', '10. Are you currently taking any hormone replacement therapy or birth control pills?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected Responses: {'Q1': {'question': '1. Have you noticed any changes in the size or shape of your breasts since your last check-up?', 'response': \"\\n\\nPatient's response: Yes, I feel like my left breast has gotten slightly larger and my right breast feels more lumpy than before.\"}, 'Q2': {'question': '2. Do you experience any pain or discomfort in your breasts?', 'response': \"\\n\\nPatient's response: Yes, I have been experiencing some pain in my left breast for the past few days.\"}, 'Q3': {'question': '3. Have you noticed any lumps or bumps in your breasts that were not there before?', 'response': \"\\n\\nResponse: No, I haven't noticed any lumps or bumps in my breasts.\"}, 'Q4': {'question': '4. Are you aware of any family history of breast cancer?', 'response': '\\n\\nRESPONSE: Yes, my mother was diagnosed with breast cancer when she was 45 years old.'}, 'Q5': {'question': '5. Do you perform regular breast self-examinations?', 'response': ' \\n\\nRESPONSE: Yes, I do perform regular breast self-examinations.'}, 'Q6': {'question': '6. Have you noticed any changes in the appearance of your nipples, such as inversion or discharge?', 'response': \"\\n\\nPatient's response: No, I haven't noticed any changes in my nipples.\"}, 'Q7': {'question': '7. Have you noticed any changes in skin texture or color on your breasts?', 'response': \"\\n\\nPatient: No, I haven't noticed any changes in skin texture or color on my breasts.\"}, 'Q8': {'question': '8. Do you experience any persistent itching or rash on your breasts?', 'response': '\\n\\nPatient: No, I do not experience any persistent itching or rash on my breasts.'}, 'Q9': {'question': '9. Have you experienced any changes in your menstrual cycle or menopausal symptoms?', 'response': \"\\n\\nI'm sorry, I cannot enter a response as I am a text-based AI assistant.\"}, 'Q10': {'question': '10. Are you currently taking any hormone replacement therapy or birth control pills?', 'response': '\\n\\nPatient: Yes, I am currently taking birth control pills.'}}\n"
     ]
    }
   ],
   "source": [
    "class QuestionAskingAgent:\n",
    "    def __init__(self, questions, llm):\n",
    "        self.questions = questions\n",
    "        self.llm = llm\n",
    "        self.responses = {}\n",
    "\n",
    "    def ask_questions(self):\n",
    "        # Parcourir chaque question et collecter la réponse\n",
    "        for i, question in enumerate(self.questions):\n",
    "            prompt_template = PromptTemplate(\n",
    "                input_variables=[\"question\"],\n",
    "                template=\"You are a medical assistant. Ask the patient the following question and wait for their response:\\n\\nQUESTION: {question}\\n\\nPlease enter your response below.\"\n",
    "            )\n",
    "            chain = LLMChain(prompt=prompt_template, llm=self.llm)\n",
    "            response = chain.run({\"question\": question})\n",
    "            self.responses[f\"Q{i+1}\"] = {\"question\": question, \"response\": response}\n",
    "        return self.responses\n",
    "\n",
    "# Création de l'agent pour poser les questions\n",
    "agent1 = QuestionAskingAgent(questions, llm)\n",
    "responses = agent1.ask_questions()\n",
    "\n",
    "# Affichage des réponses collectées\n",
    "print(\"Collected Responses:\", responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Medical Report: \n",
      "Observation:\n",
      "The patient has reported a slight increase in size in her left breast and a lumpiness in her right breast since her last check-up.\n",
      "\n",
      "Pain:\n",
      "The patient has been experiencing some pain in her left breast for the past few days.\n",
      "\n",
      "Skin Changes:\n",
      "There have been no reported changes in the appearance or texture of the patient's breast skin.\n",
      "\n",
      "Lumps:\n",
      "The patient has not noticed any lumps or masses during self-examination.\n",
      "\n",
      "Nipple Discharge:\n",
      "There have been no reported changes in nipple appearance or discharge.\n",
      "\n",
      "Appearance:\n",
      "The patient has not noticed any changes in the overall appearance of her breasts.\n",
      "\n",
      "Family History:\n",
      "The patient's mother was diagnosed with breast cancer at the age of 45, indicating a family history of breast cancer.\n",
      "\n",
      "Self-Examination:\n",
      "The patient reports performing regular breast self-examinations.\n",
      "\n",
      "Tenderness:\n",
      "The patient has reported tenderness in her left breast.\n",
      "\n",
      "Lymph Nodes:\n",
      "There have been no reported changes in the size or appearance of the lymph nodes in the underarm area.\n",
      "\n",
      "Overall, the patient has reported a slight increase in size and lumpiness in her breasts, along with tenderness and pain in her left breast. The patient also has a family history of breast cancer and is currently taking birth control pills. Regular breast\n"
     ]
    }
   ],
   "source": [
    "class MedicalReportAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate_report(self, responses):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"responses\"],\n",
    "            template=\"\"\"You are a medical assistant AI. Based on the following responses to medical questions, generate a detailed and structured medical report. The report should be formatted in the following sections:\n",
    "\n",
    "            1. **Observation**: Summarize any significant changes in the patient's condition, including breast size, shape, or any related symptoms.\n",
    "            2. **Pain**: Mention any pain or discomfort reported by the patient.\n",
    "            3. **Skin Changes**: Note any changes in the appearance or texture of the breast skin.\n",
    "            4. **Lumps**: Summarize any lumps or masses felt by the patient during self-examination.\n",
    "            5. **Nipple Discharge**: Mention if there is any nipple discharge or changes in nipple appearance.\n",
    "            6. **Appearance**: Summarize any changes in the overall appearance of the breasts, such as dimpling or puckering.\n",
    "            7. **Family History**: If relevant, include the patient's family history of breast cancer.\n",
    "            8. **Self-Examination**: Note whether the patient has been performing regular breast self-examinations.\n",
    "            9. **Tenderness**: Summarize any tenderness or sensitivity reported by the patient.\n",
    "            10. **Lymph Nodes**: Mention any changes in the size or appearance of the lymph nodes in the underarm area.\n",
    "\n",
    "            **Patient Responses**:\n",
    "            {responses}\n",
    "\n",
    "            **Generated Medical Report**:\n",
    "            \"\"\"\n",
    "        )\n",
    "        chain = LLMChain(prompt=prompt_template, llm=self.llm)\n",
    "        responses_text = \"\\n\".join([f\"{key}: {value['question']} - {value['response']}\" for key, value in responses.items()])\n",
    "        report = chain.run({\"responses\": responses_text})\n",
    "        return report\n",
    "\n",
    "# Création de l'agent pour générer le rapport médical\n",
    "agent2 = MedicalReportAgent(llm)\n",
    "medical_report = agent2.generate_report(responses)\n",
    "\n",
    "# Affichage du rapport généré\n",
    "print(\"Generated Medical Report:\", medical_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 00:22:42.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.491 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:42.492 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.607 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.610 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.612 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.613 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-22 00:22:43.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Classe de l'agent qui pose les questions et recueille les réponses\n",
    "class QuestionAskingAgent:\n",
    "    def __init__(self, questions, llm):\n",
    "        self.questions = questions\n",
    "        self.llm = llm\n",
    "        self.responses = {}\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are a medical assistant with a friendly and warm tone. Ask the patient the following question in a warm, friendly, and engaging way:\n",
    "            \n",
    "            QUESTION: {question}\n",
    "\n",
    "            Please ask the patient to respond below in a conversational tone.\"\"\"\n",
    "        )\n",
    "        chain = LLMChain(prompt=prompt_template, llm=self.llm)\n",
    "        response = chain.run({\"question\": question})\n",
    "        return response\n",
    "\n",
    "    def run_chat(self):\n",
    "        # Créer une session de chat\n",
    "        st.title(\"Breast Cancer Monitoring Chatbot\")\n",
    "        st.write(\"Hello! I'm here to ask you some questions to help with your breast health monitoring. 😊\")\n",
    "\n",
    "        # Session de chat: initialisation\n",
    "        if 'question_index' not in st.session_state:\n",
    "            st.session_state.question_index = 0\n",
    "            st.session_state.responses = {}\n",
    "\n",
    "        # Récupérer la question courante\n",
    "        question = self.questions[st.session_state.question_index]\n",
    "\n",
    "        # Afficher la question et attendre la réponse\n",
    "        question_to_ask = self.ask_question(question)\n",
    "        st.write(f\"**Question {st.session_state.question_index + 1}:** {question_to_ask}\")\n",
    "        response = st.text_input(f\"Your response to question {st.session_state.question_index + 1}:\")\n",
    "\n",
    "        if response:\n",
    "            # Enregistrer la réponse dans le dictionnaire\n",
    "            st.session_state.responses[f\"Q{st.session_state.question_index + 1}\"] = {\"question\": question, \"response\": response}\n",
    "            st.write(f\"Thanks for your response: {response}\")\n",
    "\n",
    "            # Passer à la question suivante\n",
    "            if st.session_state.question_index < len(self.questions) - 1:\n",
    "                st.session_state.question_index += 1\n",
    "            else:\n",
    "                st.write(\"Thank you for answering all the questions! 😊\")\n",
    "                # Optionnel: Générer un rapport ou traiter les réponses\n",
    "                st.write(\"Here are your responses:\")\n",
    "                for i in range(len(self.questions)):\n",
    "                    st.write(f\"**Question {i + 1}:** {self.questions[i]}\")\n",
    "                    st.write(f\"**Answer:** {st.session_state.responses[f'Q{i + 1}']['response']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Création de l'agent et démarrage du chatbot\n",
    "agent = QuestionAskingAgent(questions, llm)\n",
    "agent.run_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: 10. Are you currently taking any hormone replacement therapy or birth control pills?\n",
      "Reformulated Question: Are you currently using any form of hormone replacement therapy or taking birth control pills for the purpose of regulating your hormones or preventing pregnancy?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class SingleQuestionReformulationAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def reformulate_question(self, question):\n",
    "        # Définir le prompt pour reformuler une question\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"Reformulate the following question to make it clearer and more precise:\\n\\nORIGINAL QUESTION: {question}\\n\\nREFORMULATED QUESTION:\"\n",
    "        )\n",
    "        chain = LLMChain(prompt=prompt_template, llm=self.llm)\n",
    "        reformulated_question = chain.run({\"question\": question})\n",
    "        return reformulated_question.strip()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Remplacez `llm` par votre instance de modèle LLM\n",
    "agent = SingleQuestionReformulationAgent(llm)\n",
    "\n",
    "questions = ['1. Have you noticed any changes in the size or shape of your breasts since your last check-up?', '2. Do you experience any pain or discomfort in your breasts?', '3. Have you noticed any lumps or bumps in your breasts that were not there before?', '4. Are you aware of any family history of breast cancer?', '5. Do you perform regular breast self-examinations?', '6. Have you noticed any changes in the appearance of your nipples, such as inversion or discharge?', '7. Have you noticed any changes in skin texture or color on your breasts?', '8. Do you experience any persistent itching or rash on your breasts?', '9. Have you experienced any changes in your menstrual cycle or menopausal symptoms?', '10. Are you currently taking any hormone replacement therapy or birth control pills?']\n",
    "for question in questions :\n",
    "    reformulated_question = agent.reformulate_question(question)\n",
    "\n",
    "# Affichage de la question reformulée\n",
    "print(\"Original Question:\", question)\n",
    "print(\"Reformulated Question:\", reformulated_question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_environnement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
